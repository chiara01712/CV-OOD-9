{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOyEImd3X22nQWhNsBBPAj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "#basic config\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 32\n",
        "num_classes = 101\n",
        "input_size = 224\n",
        "feature_extract = True\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "# Paths\n",
        "\n",
        "data_root = \"./food-101/images\"\n",
        "train_meta = \"./food-101/meta/train.txt\"\n",
        "test_meta = \"./food-101/meta/test.txt\"\n",
        "\n",
        "#Loading and modification of model EfficientNetV2-M\n",
        "model_ft = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.DEFAULT)\n",
        "\n",
        "# Congela tutti i layer tranne il classificatore\n",
        "for param in model_ft.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modifica il classificatore finale\n",
        "model_ft.classifier[1] = nn.Linear(model_ft.classifier[1].in_features, num_classes)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#Ottimizzatore, loss e lancio del training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=128)\n",
        "test_loader=torch.utils.data.DataLoader(testdata, batch_size=64)\n",
        "\n",
        "prog_schedule = [\n",
        "    {'image_size': 128, 'epochs': 10, 'aug_strength': 'light', 'dropout': 0.1},\n",
        "    {'image_size': 160, 'epochs': 10, 'aug_strength': 'medium', 'dropout': 0.2},\n",
        "    {'image_size': 224, 'epochs': 20, 'aug_strength': 'strong', 'dropout': 0.3},\n",
        "]\n",
        "\n",
        "\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "class Food101Dataset(Dataset):\n",
        "    def __init__(self, root_dir, meta_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read the meta file lines\n",
        "        with open(meta_file, 'r') as f:\n",
        "            lines = f.read().splitlines()\n",
        "\n",
        "        self.samples = []\n",
        "        self.class_to_idx = {}\n",
        "        idx = 0\n",
        "        for line in lines:\n",
        "            # line like 'apple_pie/997124'\n",
        "            cls, img_id = line.split('/')\n",
        "            if cls not in self.class_to_idx:\n",
        "                self.class_to_idx[cls] = idx\n",
        "                idx += 1\n",
        "            img_path = os.path.join(root_dir, cls, img_id + '.jpg')\n",
        "            self.samples.append((img_path, self.class_to_idx[cls]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Preprocessing\n",
        "def get_train_transforms(image_size, aug_strength):\n",
        "    aug_list = [transforms.RandomHorizontalFlip()]\n",
        "    if aug_strength in ['medium', 'strong']:\n",
        "        aug_list += [transforms.ColorJitter(0.2, 0.2, 0.2, 0.1)]\n",
        "    if aug_strength == 'strong':\n",
        "        aug_list += [transforms.RandomRotation(30)]\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        *aug_list,\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "def get_test_transforms(image_size):\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "\n",
        "#DATABASE LOADING (CAMBIA DIR)\n",
        "\n",
        "# Load using custom dataset class\n",
        "full_train_dataset = Food101Dataset(data_root, train_meta, transform=None)\n",
        "test_dataset = Food101Dataset(data_root, test_meta, transform=None)\n",
        "\n",
        "\n",
        "train_size = int(0.9 * len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "            if phase == 'test' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "    print(f\"Best test Acc: {best_acc:.4f}\")\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Progressive learning stages\n",
        "prog_schedule = [\n",
        "    {'image_size': 128, 'epochs': 10, 'aug_strength': 'light', 'dropout': 0.1},\n",
        "    {'image_size': 160, 'epochs': 10, 'aug_strength': 'medium', 'dropout': 0.2},\n",
        "    {'image_size': 224, 'epochs': 20, 'aug_strength': 'strong', 'dropout': 0.3},\n",
        "]\n",
        "\n",
        "# Load EfficientNetV2\n",
        "model_ft = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.DEFAULT)\n",
        "model_ft.classifier[1] = nn.Linear(model_ft.classifier[1].in_features, num_classes)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Progressive training loop\n",
        "for stage in prog_schedule:\n",
        "    image_size = stage['image_size']\n",
        "    aug_strength = stage['aug_strength']\n",
        "    dropout = stage['dropout']\n",
        "    print(f\"Training with image size {image_size} and augmentation {aug_strength}\")\n",
        "\n",
        "\n",
        "    # Update dropout dynamically\n",
        "    model_ft.classifier[0].p = dropout\n",
        "\n",
        "    # Update transforms dynamically\n",
        "    train_dataset.dataset.transform = get_train_transforms(image_size, aug_strength)\n",
        "    val_dataset.dataset.transform = get_test_transforms(image_size)\n",
        "    test_dataset.transform = get_test_transforms(image_size)\n",
        "\n",
        "    dataloaders_dict = {\n",
        "        'train': DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4),\n",
        "        'val': DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4),\n",
        "        'test': DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "    }\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_ft.parameters()), lr=learning_rate)\n",
        "\n",
        "    # Train model\n",
        "    model_ft = train_model(model_ft, dataloaders_dict, criterion, optimizer, num_epochs=stage['epochs'])\n",
        "\n",
        "# Save the final model\n",
        "torch.save(model_ft.state_dict(), 'efficientnetv2_food101_progressive.pth')\n",
        "\n",
        "# After training is done\n",
        "test_loader = dataloaders_dict['test']\n",
        "model_ft.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model_ft(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Final Test Accuracy: {correct / total:.4f}\")\n"
      ],
      "metadata": {
        "id": "GG7FjCU5q4Zj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
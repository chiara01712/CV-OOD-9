{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvsmbGEIJbd9Z3Hw1IK1nY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiara01712/CV-OOD-9/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "586kVokcCTHD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import SVHN, CIFAR10, DTD, Places365\n",
        "from sklearn.covariance import EmpiricalCovariance\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score, accuracy_score\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==================== CONFIGURATION ====================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 101\n",
        "batch_size = 64\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "data_root = \"./food-101/images\"\n",
        "train_meta = \"./food-101/meta/train.txt\"\n",
        "test_meta = \"./food-101/meta/test.txt\"\n",
        "num_seeds = 3  # For reproducibility and statistical significance\n",
        "\n",
        "# ==================== DATASET CLASS ====================\n",
        "class Food101Dataset():\n",
        "    def __init__(self, root_dir, meta_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        self.class_to_idx = {}\n",
        "        idx = 0\n",
        "\n",
        "        with open(meta_file, 'r') as f:\n",
        "            lines = f.read().splitlines()\n",
        "\n",
        "        for line in lines:\n",
        "            cls, img_id = line.split('/')\n",
        "            if cls not in self.class_to_idx:\n",
        "                self.class_to_idx[cls] = idx\n",
        "                idx += 1\n",
        "            path = os.path.join(root_dir, cls, img_id + '.jpg')\n",
        "            self.samples.append((path, self.class_to_idx[cls]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# ==================== TRANSFORMS ====================\n",
        "def get_train_transforms(image_size, aug_strength):\n",
        "    aug_list = [transforms.RandomHorizontalFlip()]\n",
        "    if aug_strength in ['medium', 'strong']:\n",
        "        aug_list += [transforms.ColorJitter(0.2, 0.2, 0.2, 0.1)]\n",
        "    if aug_strength == 'strong':\n",
        "        aug_list += [transforms.RandomRotation(15),\n",
        "                    ]\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        *aug_list,\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "def get_test_transforms(image_size):\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "# ==================== MODEL TRAINING ====================\n",
        "\n",
        "def test_model(model, dataloaders, criterion):\n",
        "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    target_all = []\n",
        "    predicted_all = []\n",
        "    batch_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batch_count = 0\n",
        "        for data, target in dataloaders:\n",
        "            batch_count += 1\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Get the predicted classes for this batch\n",
        "            output = model(data)\n",
        "\n",
        "            # Calculate the loss for this batch\n",
        "            test_loss += criterion(output, target).item()\n",
        "\n",
        "            # Calculate the accuracy for this batch\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            correct += torch.sum(target==predicted).item()\n",
        "\n",
        "            # Append scalar target and predicted values after moving into cpu\n",
        "            target_all.extend(target.cpu().numpy())\n",
        "            predicted_all.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Calculate the average loss and total accuracy for this epoch\n",
        "    avg_loss = test_loss/batch_count\n",
        "    print(f'Validation set: Average loss: {avg_loss}, Accuracy: {correct}/{len(dataloaders.dataset)} ({100. * correct / len(dataloaders.dataset)}%)')\n",
        "\n",
        "    avg_loss = test_loss / batch_count\n",
        "    accuracy = round((accuracy_score(target_all, predicted_all)*100), 2)\n",
        "    precision = round(precision_score(target_all, predicted_all, average='weighted'), 2)\n",
        "    recall = round(recall_score(target_all, predicted_all, average='weighted'), 2)\n",
        "    f1 = round(f1_score(target_all, predicted_all, average='weighted'), 2)\n",
        "\n",
        "    print(f'Validation set: Average loss: {avg_loss:.4f}, Accuracy: {accuracy}%')\n",
        "    print(f'Weighted precision: {precision}, recall: {recall}, f1: {f1}')\n",
        "\n",
        "    return avg_loss, accuracy, precision, recall, f1\n",
        "\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            batch_count = 0\n",
        "            scaler = GradScaler()\n",
        "\n",
        "\n",
        "            for inputs, labels in tqdm(dataloaders['train'],  desc='train'):\n",
        "                optimizer.zero_grad()\n",
        "                #Reduces memory usage and speeds up training by 1.5-3x with minimal accuracy loss.\n",
        "                with autocast():\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "\n",
        "                # Push the data forward through the model layers\n",
        "                output = model(data)\n",
        "                _, preds = torch.max(output, 1)\n",
        "                loss = criterion(output, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                batch_count += 1\n",
        "\n",
        "            if batch_count % 30 == 0:\n",
        "                print(f'Batch {batch_count}/{len(dataloaders[\"train\"])} - Loss: {loss.item():.4f}')\n",
        "            epoch_loss = running_loss / len(dataloaders['train'].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders['train'].dataset)\n",
        "\n",
        "\n",
        "             # Validation phase\n",
        "              val_loss, val_acc, val_prec, val_rec, val_f1 = test_model(model, dataloaders['test'], criterion)\n",
        "\n",
        "              if val_acc > best_acc:\n",
        "                  best_acc = val_acc\n",
        "                  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                  print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# ==================== OOD DETECTION METHODS ====================\n",
        "class OODDetector:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.features = []\n",
        "        self.hook = model.features[-1].register_forward_hook(self._hook_fn)\n",
        "\n",
        "    def _hook_fn(self, module, input, output):\n",
        "        self.features.append(output)\n",
        "\n",
        "    def _get_features(self, x):\n",
        "        self.features.clear()\n",
        "        _ = self.model(x)\n",
        "        return torch.nn.functional.adaptive_avg_pool2d(self.features[-1], 1).squeeze()\n",
        "\n",
        "    def energy_score(self, logits):\n",
        "        return -torch.logsumexp(logits, dim=1)\n",
        "\n",
        "    def mahalanobis_score(self, x, class_means, precision):\n",
        "        feats = self._get_features(x).cpu().numpy()\n",
        "        if feats.ndim == 1:\n",
        "            feats = feats[None, :]\n",
        "        scores = []\n",
        "        for f in feats:\n",
        "            dists = [(f - mu).T @ precision @ (f - mu) for mu in class_means.values()]\n",
        "            scores.append(min(dists))  # oppure max(dists), dipende dallo scopo\n",
        "        return np.array(scores)\n",
        "\n",
        "    def gradient_score(self, x):\n",
        "        x = x.to(device).requires_grad_(True)\n",
        "        outputs = self.model(x)\n",
        "        targets = outputs.max(1)[0]\n",
        "        gradients = torch.autograd.grad(outputs=targets.sum(), inputs=x,\n",
        "                                      create_graph=False, retain_graph=False)[0]\n",
        "        return gradients.norm(p=2, dim=(1, 2, 3)).cpu().numpy()\n",
        "\n",
        "    def cores_score(self, x):\n",
        "        features = []\n",
        "        hooks = []\n",
        "\n",
        "        # Register hooks on intermediate layers\n",
        "        for layer in [self.model.features[4], self.model.features[8], self.model.features[-1]]:\n",
        "            hooks.append(layer.register_forward_hook(lambda m, i, o: features.append(o)))\n",
        "\n",
        "        _ = self.model(x)\n",
        "\n",
        "        # Compute CORES score\n",
        "        score = 0\n",
        "        for feat in features:\n",
        "            feat = feat.mean(dim=[2, 3])  # Global average pooling\n",
        "            score += (feat ** 2).mean(dim=1)  # L2 deviation\n",
        "\n",
        "        # Remove hooks\n",
        "        for h in hooks:\n",
        "            h.remove()\n",
        "\n",
        "        return score.cpu().numpy()\n",
        "\n",
        "# ==================== CALIBRATION METRICS ====================\n",
        "def expected_calibration_error(model, dataloader, bins=15):\n",
        "    bin_boundaries = torch.linspace(0, 1, bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    confidences = []\n",
        "    accuracies = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "            confidence, preds = torch.max(probabilities, 1)\n",
        "            accuracy = preds.eq(y)\n",
        "\n",
        "            confidences.extend(confidence.cpu())\n",
        "            accuracies.extend(accuracy.cpu())\n",
        "\n",
        "    confidences = torch.tensor(confidences)\n",
        "    accuracies = torch.tensor(accuracies)\n",
        "\n",
        "    ece = 0.0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
        "        prop_in_bin = in_bin.float().mean()\n",
        "        if prop_in_bin > 0:\n",
        "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "    return ece.item()\n",
        "\n",
        "# ==================== MAIN EXPERIMENT ====================\n",
        "def main():\n",
        "    # Seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Load datasets\n",
        "\n",
        "    train_dataset = Food101Dataset(data_root, train_meta, transform=None)\n",
        "    test_dataset = Food101Dataset(data_root, test_meta, transform=None)\n",
        "\n",
        "    # Initialize test transform\n",
        "    test_transform = get_test_transforms(224)\n",
        "    # OOD datasets\n",
        "    ood_datasets = {\n",
        "        \"SVHN\": SVHN(root='./data', split='test', download=True, transform=test_transform),\n",
        "        \"CIFAR10\": CIFAR10(root='./data', train=False, download=True, transform=test_transform),\n",
        "        \"DTD\": DTD(root='./data', split='test', download=True, transform=test_transform),\n",
        "        \"Places365\": Places365(root='./data', split='val', download=True, transform=test_transform),\n",
        "        \"Gaussian Noise\": TensorDataset(torch.randn(1000, 3, 224, 224), torch.zeros(1000))\n",
        "    }\n",
        "\n",
        "    # Progressive training schedule\n",
        "    prog_schedule = [\n",
        "        {'image_size': 128, 'epochs': 5, 'aug_strength': 'light', 'dropout': 0.2},\n",
        "        {'image_size': 160, 'epochs': 5, 'aug_strength': 'medium', 'dropout': 0.2},\n",
        "        {'image_size': 224, 'epochs': 10, 'aug_strength': 'strong', 'dropout': 0.5},\n",
        "    ]\n",
        "\n",
        "    # Results storage\n",
        "    all_results = {name: [] for name in [\"Energy\", \"Mahalanobis\", \"Gradient\", \"CORES\"]}\n",
        "\n",
        "    for seed in range(num_seeds):\n",
        "        print(f\"\\n=== Experiment Seed {seed + 1}/{num_seeds} ===\")\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # Initialize model\n",
        "        model = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.DEFAULT)\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "        model = model.to(device)\n",
        "\n",
        "        # Progressive training\n",
        "        for stage in prog_schedule:\n",
        "            print(f\"\\nStage: {stage['image_size']}px\")\n",
        "\n",
        "            # Update transforms\n",
        "            train_dataset.dataset.transform = get_train_transforms(\n",
        "                stage['image_size'], stage['aug_strength'])\n",
        "            test_dataset.dataset.transform = get_test_transforms(stage['image_size'])\n",
        "\n",
        "            # Update dropout\n",
        "            model.classifier[0].p = stage['dropout']\n",
        "\n",
        "            # Dataloaders\n",
        "            dataloaders = {\n",
        "                'train': DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True),\n",
        "                'test': DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "            }\n",
        "\n",
        "            # Unfreeze layers in later stages\n",
        "            if stage['image_size'] >= 160:\n",
        "                for param in model.features[-3:].parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            model = train_model(model, dataloaders, criterion, optimizer, stage['epochs'])\n",
        "\n",
        "        # Save model\n",
        "        torch.save(model.state_dict(), f'efficientnetv2_food101_seed{seed}.pth')\n",
        "\n",
        "        # Evaluate calibration\n",
        "        ece = expected_calibration_error(model, dataloaders['test'])\n",
        "        print(f\"\\nExpected Calibration Error: {ece:.4f}\")\n",
        "\n",
        "        # OOD Detection Setup\n",
        "        detector = OODDetector(model)\n",
        "\n",
        "        # Compute class means and covariance for Mahalanobis\n",
        "        print(\"Computing class statistics...\")\n",
        "        train_feats = []\n",
        "        train_labels = []\n",
        "        with torch.no_grad():\n",
        "            for x, y in dataloaders['train']:\n",
        "                x = x.to(device)\n",
        "                feats = detector._get_features(x).cpu().numpy()\n",
        "                train_feats.append(feats)\n",
        "                train_labels.append(y.numpy())\n",
        "\n",
        "        train_feats = np.vstack(train_feats)\n",
        "        train_labels = np.concatenate(train_labels)\n",
        "\n",
        "        class_means = {}\n",
        "        for c in np.unique(train_labels):\n",
        "            class_means[c] = train_feats[train_labels == c].mean(axis=0)\n",
        "\n",
        "        cov = EmpiricalCovariance().fit(train_feats)\n",
        "        precision = cov.precision_\n",
        "\n",
        "        # Evaluate OOD detection\n",
        "        test_loader = dataloaders['test']\n",
        "\n",
        "        for ood_name, ood_data in ood_datasets.items():\n",
        "            print(f\"\\nEvaluating on {ood_name}...\")\n",
        "            ood_loader = DataLoader(ood_data, batch_size=batch_size)\n",
        "\n",
        "            # Get all scores\n",
        "            methods = {\n",
        "                \"Energy\": (detector.energy_score, False),\n",
        "                \"Mahalanobis\": (lambda x: detector.mahalanobis_score(x, class_means, precision), True),\n",
        "                \"Gradient\": (detector.gradient_score, True),\n",
        "                \"CORES\": (detector.cores_score, True)\n",
        "            }\n",
        "\n",
        "            for method_name, (score_fn, higher_is_ood) in methods.items():\n",
        "                # Compute ID scores\n",
        "                id_scores = []\n",
        "                with torch.no_grad():\n",
        "                    for x, _ in test_loader:\n",
        "                        x = x.to(device)\n",
        "                        if method_name == \"Energy\":\n",
        "                            logits = model(x)\n",
        "                            scores = score_fn(logits).cpu().numpy()\n",
        "                        else:\n",
        "                            scores = score_fn(x)\n",
        "                        id_scores.extend(scores)\n",
        "\n",
        "                # Compute OOD scores\n",
        "                ood_scores = []\n",
        "                with torch.no_grad():\n",
        "                    for x, _ in ood_loader:\n",
        "                        x = x[0].to(device) if isinstance(x, list) else x.to(device)\n",
        "                        if method_name == \"Energy\":\n",
        "                            logits = model(x)\n",
        "                            scores = score_fn(logits).cpu().numpy()\n",
        "                        else:\n",
        "                            scores = score_fn(x)\n",
        "                        ood_scores.extend(scores)\n",
        "\n",
        "                # Evaluate\n",
        "                labels = np.concatenate([np.zeros_like(id_scores), np.ones_like(ood_scores)])\n",
        "                scores = np.concatenate([id_scores, ood_scores])\n",
        "\n",
        "                if not higher_is_ood:\n",
        "                    scores = -scores\n",
        "\n",
        "                auroc = roc_auc_score(labels, scores)\n",
        "                precision_pr, recall_pr, _ = precision_recall_curve(labels, scores)\n",
        "                aupr = auc(recall_pr, precision_pr)\n",
        "                threshold = np.percentile(scores[labels == 1], 95)\n",
        "                fpr95 = np.mean(scores[labels == 0] >= threshold)\n",
        "\n",
        "                print(f\"{method_name}: AUROC={auroc:.4f}, AUPR={aupr:.4f}, FPR95={fpr95:.4f}\")\n",
        "                all_results[method_name].append((auroc, aupr, fpr95))\n",
        "\n",
        "    # Print final results\n",
        "    print(\"\\n=== FINAL RESULTS ===\")\n",
        "    for method, results in all_results.items():\n",
        "        aurocs = [r[0] for r in results]\n",
        "        auprs = [r[1] for r in results]\n",
        "        fpr95s = [r[2] for r in results]\n",
        "\n",
        "        print(f\"\\n{method}:\")\n",
        "        print(f\"AUROC: {np.mean(aurocs):.4f} ± {np.std(aurocs):.4f}\")\n",
        "        print(f\"AUPR: {np.mean(auprs):.4f} ± {np.std(auprs):.4f}\")\n",
        "        print(f\"FPR95: {np.mean(fpr95s):.4f} ± {np.std(fpr95s):.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}